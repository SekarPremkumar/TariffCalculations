{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in our LLAMA_CLOUD_API_KEY\n",
    "\n",
    "import nest_asyncio\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import AzureOpenAI\n",
    "from typing import Optional, Dict\n",
    "from smolagents.models import OpenAIServerModel\n",
    "from smolagents import CodeAgent, DuckDuckGoSearchTool, VisitWebpageTool\n",
    "\n",
    "\n",
    "from prompt_library import decoding_manual_prompt, logical_questions_prompt, logical_answer_prompt,agent_prompt,enhancing_vessel_details_prompt\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "def parse_document(file_name, llama_cloud_api_key):\n",
    "    \n",
    "    # bring in deps\n",
    "    from llama_cloud_services import LlamaParse\n",
    "    from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "    # set up parser\n",
    "    parser = LlamaParse(\n",
    "        api_key=llama_cloud_api_key,\n",
    "        result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    "    )\n",
    "\n",
    "    # use SimpleDirectoryReader to parse our file\n",
    "    file_extractor = {\".pdf\": parser}\n",
    "    documents = SimpleDirectoryReader(input_files=[file_name], file_extractor=file_extractor).load_data()\n",
    "    contents=[documents[int(id)].text_resource.text for id in range(2,len(documents))]\n",
    "    return contents\n",
    "\n",
    "\n",
    "def creating_index(contents):\n",
    "    # Step 1: Load documents\n",
    "    documents = contents\n",
    "\n",
    "    \n",
    "    model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model_embed.encode(documents, convert_to_numpy=True)\n",
    "\n",
    "    # Step 3: Normalize embeddings for cosine similarity\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    # Step 4: Use FAISS inner product index (cosine similarity)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, \"my_faiss_index.index\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def search_manual_index(query):\n",
    "\n",
    "    index = faiss.read_index(\"my_faiss_index.index\")\n",
    "    model_embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    # Step 5: Normalize the query and search\n",
    "    query =query\n",
    "    query_embedding = model_embed.encode([query], convert_to_numpy=True)\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "\n",
    "    D, I = index.search(query_embedding, k=1)\n",
    "    return I[0].item()\n",
    "\n",
    "def decode_manual(client,prompt,model,manual):\n",
    "    prompt=prompt.format(manual=manual)\n",
    "    \n",
    "    # print(prompt)\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    instructions=response.choices[0].message.content\n",
    "    return instructions\n",
    "\n",
    "def logical_questions_generation(client,prompt,model,manual):\n",
    "    prompt=prompt.format(manual=manual)\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    questions=response.choices[0].message.content\n",
    "    return questions\n",
    "    \n",
    "\n",
    "def logical_answers_generation(client,prompt,model,vessel_details, questions):\n",
    "    prompt=prompt.format(questions=questions,vessel_details=vessel_details)\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    questions=response.choices[0].message.content\n",
    "    return questions\n",
    "\n",
    "def initiate_openai_model(openai_api_key,api_version,azure_endpoint):\n",
    "    client = AzureOpenAI(\n",
    "    azure_endpoint = azure_endpoint,\n",
    "    api_key=openai_api_key ,\n",
    "    api_version=api_version\n",
    "    )\n",
    "    return client\n",
    "\n",
    "def tariff_calculator_agent(model_id,api_key,api_version,azure_endpoint, agent_prompt, vessel_details,instructions,answer):\n",
    "    \n",
    "    class AzureOpenAIServerModel(OpenAIServerModel):\n",
    "        \"\"\"This model connects to an Azure OpenAI deployment.\n",
    "\n",
    "        Parameters:\n",
    "            model_id (`str`):\n",
    "                The model identifier to use on the server (e.g. \"gpt-3.5-turbo\").\n",
    "            azure_endpoint (`str`, *optional*):\n",
    "                The Azure endpoint, including the resource, e.g. `https://example-resource.azure.openai.com/`\n",
    "            api_key (`str`, *optional*):\n",
    "                The API key to use for authentication.\n",
    "            custom_role_conversions (`Dict{str, str]`, *optional*):\n",
    "                Custom role conversion mapping to convert message roles in others.\n",
    "                Useful for specific models that do not support specific message roles like \"system\".\n",
    "            **kwargs:\n",
    "                Additional keyword arguments to pass to the Azure OpenAI API.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            model_id: str,\n",
    "            azure_endpoint: Optional[str] = None,\n",
    "            api_key: Optional[str] = None,\n",
    "            api_version: Optional[str] = None,\n",
    "            custom_role_conversions: Optional[Dict[str, str]] = None,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            super().__init__(model_id=model_id, api_key=api_key, custom_role_conversions=custom_role_conversions, **kwargs)\n",
    "            # if we've reached this point, it means the openai package is available (baseclass check) so go ahead and import it\n",
    "            import openai\n",
    "\n",
    "            self.client = openai.AzureOpenAI(\n",
    "                api_key=api_key,\n",
    "                api_version=api_version,\n",
    "                azure_endpoint=azure_endpoint\n",
    "            )\n",
    "            \n",
    "    model = AzureOpenAIServerModel(\n",
    "    model_id = model_id,\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=azure_endpoint\n",
    "    )\n",
    "    \n",
    "    agent = CodeAgent(tools=[], model=model)\n",
    "    agent_prompt=agent_prompt.format(vessel_details=vessel_details,instructions= instructions, answer=answer)\n",
    "    ans=agent.run(agent_prompt)\n",
    "    return ans\n",
    "\n",
    "def enhancing_vessel_info(client,model,questions,vessel_details,prompt):\n",
    "    prompt=prompt.format(questions=questions,vessel_details=vessel_details)\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    enhanced_vessel_details=response.choices[0].message.content\n",
    "    return enhanced_vessel_details\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    ##### Pass the necessary inputs here ##########################################\n",
    "    \n",
    "    are_you_running_code_first_time=True # If True, it will creta eand save vector index, if False, then it will use already created index.\n",
    "    file_path =\"\"\n",
    "    llama_cloud_api_key=\"\"\n",
    "    openai_api_key=\"\"\n",
    "    azure_endpoint = \"\"\n",
    "    api_version=\"\"\n",
    "    model= \"\"\n",
    "\n",
    "    vessel_details=\"\"\"\n",
    "    Calculate the different tariffs payable by the following vessel berthing at the port of Durban\n",
    "    Vessel Details:\n",
    "\n",
    "    General\n",
    "    Vessel Name: SUDESTADA\n",
    "    Built: 2010\n",
    "    Flag: MLT - Malta\n",
    "    Classification Society: Registro Italiano Navale\n",
    "    Call Sign: [Not provided]\n",
    "\n",
    "    Main Details\n",
    "    Lloyds / IMO No.: [Not provided]\n",
    "    Type: Bulk Carrier\n",
    "    DWT: 93,274\n",
    "    GT / NT: 51,300 / 31,192\n",
    "    LOA (m): 229.2\n",
    "    Beam (m): 38\n",
    "    Moulded Depth (m): 20.7\n",
    "    LBP: 222\n",
    "    Drafts SW S / W / T (m): 14.9 / 0 / 0\n",
    "    Suez GT / NT: - / 49,069\n",
    "\n",
    "    Communication\n",
    "    E-mail: [Not provided]\n",
    "    Commercial E-mail: [Not provided]\n",
    "    DRY\n",
    "    Number of Holds: 7\n",
    "    \n",
    "    Cargo Details\n",
    "    Cargo Quantity: 40,000 MT\n",
    "    Days Alongside: 3.39 days\n",
    "    Arrival Time: 15 Nov 2024 10:12\n",
    "    Departure Time: 22 Nov 2024 13:00\n",
    "\n",
    "    Activity/Operations\n",
    "    Activity: Exporting Iron Ore\n",
    "    Number of Operations: 2\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_dues=[\n",
    "    \"light dues\",\n",
    "    \"port dues\",\n",
    "    \"towage dues\",\n",
    "    \"vehicle traffic services (VTS) dues\",\n",
    "    \"pilotage dues\",\n",
    "    \"running of vessel lines dues\"\n",
    "    ]\n",
    "    \n",
    "    ######################### Input ends here #######################################\n",
    "    \n",
    "    \n",
    "    final_result={}\n",
    "    contents=parse_document(file_path,llama_cloud_api_key)\n",
    "    \n",
    "    for i in list_of_dues:\n",
    "        print(\"Calculating \"+i)\n",
    "        Query = \"I need to calculate \"+i+\" for my vessel.\"\n",
    "        if are_you_running_code_first_time:\n",
    "            creating_index(contents)\n",
    "            id=search_manual_index(Query)\n",
    "        else:\n",
    "            id=search_manual_index(Query)\n",
    "            \n",
    "        manual=contents[id]\n",
    "        # print(manual)\n",
    "        print(\"--------------------------------\")\n",
    "        openai_client=initiate_openai_model(openai_api_key,api_version,azure_endpoint)\n",
    "        \n",
    "        manual_elaboration= decode_manual(openai_client,decoding_manual_prompt, model,manual)\n",
    "        \n",
    "        questions = logical_questions_generation(openai_client, logical_questions_prompt, model, manual )\n",
    "        # print(questions)\n",
    "        \n",
    "        enhanced_vessel_details = enhancing_vessel_info(openai_client,model,questions,vessel_details,enhancing_vessel_details_prompt)\n",
    "        print(enhanced_vessel_details)\n",
    "        answers= logical_answers_generation(openai_client, logical_answer_prompt, model,enhanced_vessel_details,questions)\n",
    "        # print(answers)\n",
    "        \n",
    "        final_answer=tariff_calculator_agent(model,openai_api_key,api_version,azure_endpoint, agent_prompt,enhanced_vessel_details,manual_elaboration,answers)\n",
    "        \n",
    "        final_result[i]=final_answer\n",
    "        \n",
    "    return final_result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "DeprecationWarning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tariff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
